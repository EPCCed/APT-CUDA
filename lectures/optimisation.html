<!doctype html>
<html>
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

<title>Performance optimisation of CUDA programs</title>

<link rel="stylesheet" href="../reveal.js/dist/reset.css">
<link rel="stylesheet" href="../reveal.js/dist/reveal.css">
<link rel="stylesheet" href="../reveal.js/dist/theme/white.css" id="theme">
<link rel="stylesheet" href="../reveal.js/css/local.css">

<!-- Theme used for syntax highlighting of code -->
<link rel="stylesheet" href="../reveal.js/lib/css/vs.css">

</head>

<!-- Start of presentation --> 
<body>
<div class="reveal">
<div class="slides">

  <section>

    <h1 style = "text-transform: none !important">Performance Optimisation</h1>
    <!--
    <p>Kevin Stratford</p>
    <p> kevin@epcc.ed.ac.uk </p> -->
    <br>
    <p>Material by: Alan Gray, Kevin Stratford, Rupert Nash</p>
    <img class = "plain" src ="../reveal.js/img/epcc_logo.png" alt = "EPCC Logo" />

  </section>

  <section>
  <h3>Architecture of NVIDIA accelerated system</h3>

  <img class = "plain" src = "ag-schematic-system.png" alt = "A
  schematic diagram showing an accelerated system with CPU host and
  GPU device, the latter having SMs and cores detailed" />

  </section>

  <section>
    <h3>Sources of poor performance</h3>

    <ul>
    <li> Potential sources of poor performance include: </li>
    <ul class = "inner">
        <li> Lack of parallelism </li>
        <li> Copying data to/from device</li>
        <li> Device under-utilisation / memory latency
        <li> Memory bandwidth
        <li> Code branches
    </ul>
    <li> Possible solutions </li>
    <ul class = "inner">
       <li> Should be relevant on many GPU devices</li>
       <li> "Mileage may vary"
    </ul>
    <li> Some comments on profiling
    <ul class = "inner">
       <li> Do you have a performance problem?</li>
    </ul>
    </ul>
  </section>

  <section>
    <h4> Exposing parallelism </h4>

    <ul>
    <li> Amdahl's Law: </li>
    <ul class = "inner">
        <li> Parallel performance limited by fraction of code that is serial</li>
        <li> Applies particularly to GPUs
    </ul>
    <li> Performance relies on use of many parallel threads </li>
    <ul class = "inner">
       <li> Degree of parallelism higher than typical CPU</li>
       <li> Typically want at least $O(10^5) - O(10^6)$ threads in a kernel
    </ul>
    <li> Effort must be made to expose parallelism
    <ul class = "inner">
       <li> As much as absolutely possible </li>
       <li> Rewriting / refactoring / different algorithm</li>
    </ul>
    </ul>

  </section>

  <section>
    <h4> Copying between host and device </h4>

    <ul>
    <li> Separate memory spaces mean some copying inevitable
    <ul class = "inner">
        <li> Via PCI/Express bus
        <li> Relatively slow/expensive</li>
    </ul>
    <li> Simply must avoid unnecessary copies </li>
    <ul class = "inner">
       <li> Keep data resident on device</li>
       <li> May involve moving all relevant code to device</li>
       <li> Recalculation / extra computation instead of data communication
    </ul>
    <!-- note Device Memory allocation can also be expensive -->
    </ul>
  </section>

  <section>
    <h4> Removing data transfer </h4>

    <dl style = "font-size: 80%">

    <dt> Consider a common pattern: </dt>

    <pre><code class = "cpp" data-trim>
for (it = 0; it < nTimeSteps; it++) {
  myCheapHostOperation(hostData);
  cudaMemcpy(..., cudaMemcpyHostToDevice);
  myExpensiveKernel &lt&lt&lt...&gt&gt&gt (deviceData, ...);
  cudaMemcpy(..., cudaMemcpyDeviceToHost);
}
    </code></pre>

   <dt> Must be refactored to: </dt>

   <pre><code class = "cpp" data-trim>
cudaMemcpy(..., cudaMemcpyHostToDevice);

for (it = 0; it < nTimeSteps; it++) {
  myCheapKernel &lt&lt&lt ... &gt&gt&gt (deviceData, ...);
  myExpensiveKernel &lt&lt&lt ... &gt&gt&gt (deviceData, ...)
}

cudeMemcpy(..., cudaMemcpyDeviceToHost);
   </code></pre>
    </dl>
  </section>

  <section>
    <h4> Occupancy and latency hiding </h4>

    <ul>
      <li> Work decomposed and distributed between threads </li>
      <ul class = "inner">
        <li> Suggests may want as many threads as there are cores
        <li> ...or some cores will be left idle
      </ul>
      <li> Actually want $$ N_{threads} >> N_{cores}$$
      <br>
      <li> Latency for access to main memory
      <ul class = "inner">
        <li> Perhaps 100 clock cycles
        <li> If other threads are available, can be swapped in quickly
      </ul>

    </ul>
  </section>

  <section>
    <h4> Example </h4>

     <dl>
       <dt> Suppose we have a two-dimensional loop </dt>
     </dl>
     <pre><code class = "cpp" data-trim>
for (i = 0; i < 512; i++) {
  for (j = 0; j < 512; j++) {
    /* ... work ... */
     </code></pre>

     <dl>
       <dt> Parallelise inner loop only </dt>
       <ul class = "inner">
         <li> Can use 512 threads
         <li> Poor occupancy!
       </ul>
     </dl>

     <dl>
       <dt> Parallelise both loops </dt>
       <ul class = "inner">
         <li> Can use $512 \times 512 = 262,\!144$ threads
         <li> Much better!
       </ul>
     </dl>


  </section>

  <section>
    <h4> CPU Caching </h4>


    <pre><code class = "cpp">
/* C: recall right-most index runs fastest */
for (i = 0; i < NI; i++) {
  for (j = 0; j < NJ; j++) {
    output[i][j] = input[i][j];
  }
}
    </code></pre>

    <pre><code class = "fortran" data-trim>
! Fortran: recall left-most index runs fastest
do j = 1, NJ
  do i = 1, NI
    output(i,j) = input(i,j)
  end do
end do
    </code></pre>

    <dl style = "font-size: 80%">
      <dt> Individual thread has consecutive memory accesses </dt>
    </dl>

  </section>

  <section>
    <h4> Memory coalescing </h4>

    <ul>
      <li> GPUs have a high <i>peak</i> memory bandwidth </li>
      <ul class = "inner">
        <li> But only achieved when accesses are <i> coalesced</i>
        <li> That is, when:
      </ul>

      <p style = "text-align: center">
      consecutive threads access consecutive memory locations
      </p>

      <li> If not, access may be serialised
      <ul class = "inner">
        <li> Performance disaster
        <li> Need to refactor to allow coalescing
      </ul>
    </ul>

  </section>

  <section>
    <h4> So, what is the correct order? </h4>


    <dl style = "font-size: 80%">
      <dt> In one dimension, the picture is relatively simple </dt>
      <ul class = "inner">
        <li> <code>threadsPerBlock = (nThreads, 1, 1) </code></li>
        <li> Consective threads are those with consective index </li>
      </ul>
    </dl>    

    <pre><code class = "cpp">
/* In C: */
idx = blockIdx.x*blockDim.x + threadIdx.x;
output[idx] = input[idx];
      </code></pre>
      <pre><code class = "fortran" data-trim>
! In Fortran:
idx = (blockIdx%x - 1)*blockDim%x + threadIdx%x
output(idx) = input(idx)
      </code></pre>

      <dl style = "font-size: 80%">
        <dt>Both good; accesses are coalesced. </dt>
      </dl>
  </section>


  <section>
    <h4> Two-dimensional array: C </h4>

    <dl style = "font-size: 80%">
      <dt> Recall: right-most index runs fastest </dt>
    </dl>    

    <pre><code class = "cpp">
/* Bad: consecutive threads have strided access */
i = blockIdx.x*blockDim.x + threadIdx.x;
for (j = 0; j < NJ; j++) {
  output[i][j] = input[i][j];
}
    </code></pre>

    <pre><code class = "cpp" data-trim>
/* Good: consecutive threads have contiguous access */
j = blockIdx.x*blockDim.x + threadIdx.x;
for (i = 0; i < NI; i++) {
  output[i][j] = input[i][j];
}
    </code></pre>

  </section>

  <section>
    <h4> Two-dimensional array: Fortran </h4>

    <dl style = "font-size: 80%">
      <dt> Recall: left-most index runs fastest </dt>
    </dl>    

    <pre><code class = "fortran">
! Bad: consecutive threads have strided access
j = blockIdx.x*blockDim.x + threadIdx.x;
do i = 1,  NI
  output(i, j) = input(i, j);
end do
    </code></pre>

    <pre><code class = "fortran" data-trim>
! Good: consecutive threads have contiguous access
i = blockIdx.x*blockDim.x + threadIdx.x;
do j = 1, NJ
  output(i, j) = input(i, j);
end do
    </code></pre>

  </section>

  <section>
    <h4> Two-dimensional decomposition </h4>

    <dl style = "font-size: 80%">
      <dt> More complex </dt>
      <ul class = "inner">
        <li> <code> blocksPerGrid = (nBlocksX, nBlocksY, 1) </code>
        <li> <code> threadsPerBlock = (nThreadsX, nThreadsY, 1) </code>
        <li> x counts fastest, then y (and then z, in three dimensions)
      </ul>
    </dl>

    <pre><code class = "cpp">
/* C: note apparent transposition of i, j here... */
int j = blockIdx.x*blockDim.x + threadIdx.x;
int i = blockIdx.y*blockDim.y + threadIdx.y;

output[i][j] = input[i][j];
    </code></pre>

    <pre><code class = "fortran" data-trim>
! Fortran: looks more natural
i = (blockIdx%x - 1)*blockDim%x + threadIdx%x
j = (blockIdx%y - 1)*blockDim%y + threadIdx%y

output(i, j) = input(i, j)
    </code></pre>
  </section>

  <section>
    <h4>Grid stride loops</h4>
    <dl>
      <dt>Common advice is to parallelise a loop by mapping one
	iteration to one thread</dt>
      <dd>
	<ul>
	  <li>GPUs have limits on the number of threads (1024) and blocks
	  that make up a grid</li>

	  <li>There is a small cost to starting/ending a block</li>

	  <li>Kernels often need to compute common values which result
	  in redundant computation</li>

	  <li>Kernels which use shared memory need to initialise this</li>
	</ul>

      </dd>

      <dt>Solution: have each CUDA thread perform several
	iterations</dt>
	<dd>
	  <ul>
	    <li>But how to divide?</li>
	    <li>Memory coalescing requires a strided loop.</li>
	    <li>Flexible solution is to use the total number of
	    threads in the grid.</li>
	  </ul>
	</dd>
      </dl>
  </section>
  <section>
      <h4>Grid stride loops</h4>
      <pre><code class="cpp">
__global__ void kernel1d(int N, float* data) {
  for (int i = blockIdx.x*blockDim.x + threadIdx.x;
       i < N;
       i += blockDim.x * gridDim.x) {
    // loop body
  }
int main() {
  int devId;
  cudaGetDevice(&devId);
  int numSM;
  cudaDeviceGetAttribute(&numSM,
    cudaDevAttrMultiProcessorCount, devId);
  kernel1d<<<32*numSM, 128>>>(N, dev_data);
}
      </code></pre>
      <p style="font-size: 80%">Can also run kernel in serial for debugging</p>
      </section>

  <section>
    <h4> Code branching </h4>

    <dl style = "font-size: 80%">
      <dt> Threads are scheduled in groups of 32 </dt>
      <ul class = "inner">
        <li> Share same instruction scheduling hardware units
        <li> A group is referred to as a <i>warp</i>
        <li> Warp executes instructions in lock-step (SIMT)
      </ul>
      <dt> Branches in the code can cause serialisation </dt>
      <ul class = "inner">
         <li> Warp executes all branches taken by at least one thread
         <li> Threads that aren't in the executing branch sit idle
      </ul>
    </dl>

  </section>

  <section>
     <h4> Avoiding warp divergence </h4>

    <dl style = "font-size: 80%">
      <dt> Imagine want to split threads into two groups: </dt>
    </dl>
    <pre><code class = "cpp">
/* Bad: threads in same warp diverge... */

tid = blockIdx.x*blockDim.x + threadIdx.x;

if (tid % 2 == 0) {
  /* Threads 0, 2, 4, ... do one thing ... */
}
else {
  /* Threads 1, 3, 5, ... do something else */
}
    </code></pre>
  </section>

  <section>
    <h4> Avoiding warp divergence </h4>

    <pre><code class = "cpp" data-trim>

/* Good: threads in same warp follow same path ...
 * Note use of the internal variable "warpSize" */

tid = blockIdx.x*blockDim.x + threadIdx.x;

if ( (tid / warpSize) % 2 == 0) {
  /* Threads 0, 1, 2, 3, 4, ... do one thing ... */
}
else {
  /* Threads 32, 33, 34, 35,  ... do something else */
}
    </code></pre>

  </section>

  <section>
    <h4> Performance Problem? </h4>

    <dl style = "font-size: 80%">
      <dt> Compiler can help to prevent problems </dt>
    </dl>
    <pre><code class = "bash">
$ nvcc -Xptxas -v ...
    </code></pre>
    <dl style = "font-size: 80%">
      <dt> Verbose option for PTX stage</dt>
      <ul class = "inner">
        <li> Parallel Thread Execution assembler (an intermediate form)
        <li> Reports register usage, constant/shared memory usage in kernels
        <li> Reports spills to global memory (very harmful to performance)
        <li> Use iteratively during development
      </ul>
    </dl>
    

  </section>

  <section>
    <h4> Profiling CUDA code </h4>

    Two options:
    <dl style = "font-size: 80%">
      <dt>Nsight Systems</dt>
      <dd>
	<ul class = "inner">
	  <li><a href="https://developer.nvidia.com/nsight-systems">https://developer.nvidia.com/nsight-systems</a></li>
	  <li>Analysis of overall application performance on both CPU
	  and GPU</li>
	  <li>Unnecessary synchronisation / data movement between host and
	  device</li>
	  <li>GUI and CLI interfaces (versions must match exactly to use
	  remotely)</li>
	</ul>
      </dd>

      <dt>Nsight Compute</dt>
      <dd>
	<ul class = "inner">
	  <li><a href="https://developer.nvidia.com/nsight-compute">https://developer.nvidia.com/nsight-compute</a></li>
	  <li>Detailed profiling of kernels</li>
	  <li>Use "baseline" profile to see result of changes</li>
	  <li>Roofline analysis vs. "speed of light"</li>
	</ul>
      </dd>

    </dl>
  </section>
  <section>
    <h4>Using Nsight Systems CLI</h4>
    <ul>
      <li>You can use the GUI to run and profile programs locally, if you
      have a suitable GPU in your workstation.</li>

      <li>For remote systems usually need to:
      <ul>
	<li>run the CLI</li>
	<li>convert and reduce data</li>
	<li>copy to data to your workstation</li>
	<li>and import into the GUI</li>
      </ul>

      <pre><code class ="bash" data-trim>
#SBATCH --qos=gpu
# Further batch system directives
module load nvidia/nvhpc
nsys profile -o reconstruct-${SLURM_JOB_ID} ./reconstruct
    </code></pre>
    </li>

    <li>LOTS of options to control exactly what data is collected: see
    documentation <a
    href="https://docs.nvidia.com/nsight-systems/UserGuide/index.html">https://docs.nvidia.com/nsight-systems/UserGuide/index.html</a></li>
    </ul>
  </section>
  <section>
    <h4>Using Nsight Systems - GUI</h4>
    <p style="font-size" 80%">Install from the NVIDIA website, and
    open the <code>.qdrep</code> file.</p>

     <img class = "plain" src = "nsys-recon1.png" alt = "Screenshot of
    Nsight Systems profile of the reconstruct exercise" />
  </section>

  <section>
    <h4>Using Nsight Compute - CLI</h4>
    <ul>
      <li>Very similar to Nsight Systems in workflow, but no
      conversion step</li>
      <li>
      <pre><code class ="bash" data-trim>
#SBATCH --qos=gpu
# Further batch system directives
module load nvidia/nvhpc
ncu -o reconstruct-${SLURM_JOB_ID} reconstruct
      </code></pre>
      </li>
      <li>Will run with a modest set of metrics, OK for a first
    look.</li>
    <li>Often better to focus on a single kernel at a time and record
    more data.</li>
    <li>The GUI has various "sections" for particular analyses which
    can be enabled or use preset "section sets", e.g.
    <pre><code class ="bash" data-trim>
ncu -o reconstruct-${SLURM_JOB_ID} \
    --kernel-name 'inverseEdgeDetect' \
    --launch-skip 1 \
    --lauch-count 10 \
    --set detailed \
    reconstruct
    </code></pre></li>
    </ul>
  </section>

  <section>
    <h4>Using Nsight Compute - GUI</h4>
    <p style="font-size:80%">Install from the NVIDIA website, and open the
    <code>.ncu-rep</code> files you copy from Cirrus.</p>

    <img src="ncu-SOL.png" alt="Screenshot of Nsight Compute
    Speed-of-light summary" />
  </section>
    <section>
    <h4>Profiling a large application</h4>
    <dl style = "font-size: 80%">
    <dt>Reduce profiling to a region</dt>
    <dd>
      <ul>
	<li>Disable profiling from application start.</li>
	<li><code class="cpp">#include &lt;cuda_profiler_api.h&gt;</code></li>
	<li>Insert <code>cudaProfilerStart()</code> before region of
	interest</li>
	<li>Insert <code>cudaProfilerStop()</code> after region of
	interest</li>
      </ul>
    </dd>

    <dt>Annotate code to make analysis simpler</dt>
    <dd>
      <ul>
	<li><a
    href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#nvtx">NVIDIA
	Tools Extension</a></li>
	<li><code class="cpp">#include &lt;nvToolsExt.h&gt;</code></li>
	<li>Name threads, streams, devices, etc</li>
	<li>Mark instantaneous event: <code>nvtxMark("Custom message");</code>
	<li>Mark ranges:</li>
	<pre><code class="cpp">
void ComputeNextTimeStep() {
  nvtxRangePush("Do timestep");
  compute_kernel &lt;&lt;&lt; ... &gt;&gt;&gt;(args);
  cudaDeviceSynchronize();
  nvtxRangePop();
}
  </code></pre>
      </ul>
    </dd>
  </dl>
  </section>

  <section>
  <h4> Summary </h4>

  <dl style = "font-size: 80%">
    <dt> With care, a good fraction of peak performance is possible </dt>
    <ul class = "inner">
      <li> Can be quite difficult on CPU </li>
    </ul>
    <dt> Not all problems well suited </dt>
    <ul class = "inner">
      <li> Often just not enough parallelism</li>
    </ul>
  </dl>

  </section>

</div>
</div>

<!-- End of presentation -->

<script src="../reveal.js/dist/reveal.js"></script>
<script src="../reveal.js/plugin/math/math.js"></script>
<script src="../reveal.js/plugin/highlight/highlight.js"></script>

<script>
// More info about config & dependencies:
// - https://github.com/hakimel/reveal.js#configuration
// - https://github.com/hakimel/reveal.js#dependencies
Reveal.initialize({
  controls: false,
  slideNumber: true,
  center: false,
  plugins: [ RevealMath, RevealHighlight ]
});
</script>

</body>
</html>
