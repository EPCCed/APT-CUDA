<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <meta name="author" content="Rupert Nash, Kevin Stratford, Alan Gray">
  <title>Optimizing a CUDA application</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="../../reveal.js/dist/reset.css">
  <link rel="stylesheet" href="../../reveal.js/dist/reveal.css">
  <style>
    .reveal .sourceCode {  /* see #7635 */
      overflow: visible;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
    /* CSS for syntax highlighting */
    pre > code.sourceCode { white-space: pre; position: relative; }
    pre > code.sourceCode > span { line-height: 1.25; }
    pre > code.sourceCode > span:empty { height: 1.2em; }
    .sourceCode { overflow: visible; }
    code.sourceCode > span { color: inherit; text-decoration: inherit; }
    div.sourceCode { margin: 1em 0; }
    pre.sourceCode { margin: 0; }
    @media screen {
    div.sourceCode { overflow: auto; }
    }
    @media print {
    pre > code.sourceCode { white-space: pre-wrap; }
    pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
    }
    pre.numberSource code
      { counter-reset: source-line 0; }
    pre.numberSource code > span
      { position: relative; left: -4em; counter-increment: source-line; }
    pre.numberSource code > span > a:first-child::before
      { content: counter(source-line);
        position: relative; left: -1em; text-align: right; vertical-align: baseline;
        border: none; display: inline-block;
        -webkit-touch-callout: none; -webkit-user-select: none;
        -khtml-user-select: none; -moz-user-select: none;
        -ms-user-select: none; user-select: none;
        padding: 0 4px; width: 4em;
        color: #aaaaaa;
      }
    pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
    div.sourceCode
      {   }
    @media screen {
    pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
    }
    code span.al { color: #ff0000; font-weight: bold; } /* Alert */
    code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
    code span.at { color: #7d9029; } /* Attribute */
    code span.bn { color: #40a070; } /* BaseN */
    code span.bu { color: #008000; } /* BuiltIn */
    code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
    code span.ch { color: #4070a0; } /* Char */
    code span.cn { color: #880000; } /* Constant */
    code span.co { color: #60a0b0; font-style: italic; } /* Comment */
    code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
    code span.do { color: #ba2121; font-style: italic; } /* Documentation */
    code span.dt { color: #902000; } /* DataType */
    code span.dv { color: #40a070; } /* DecVal */
    code span.er { color: #ff0000; font-weight: bold; } /* Error */
    code span.ex { } /* Extension */
    code span.fl { color: #40a070; } /* Float */
    code span.fu { color: #06287e; } /* Function */
    code span.im { color: #008000; font-weight: bold; } /* Import */
    code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
    code span.kw { color: #007020; font-weight: bold; } /* Keyword */
    code span.op { color: #666666; } /* Operator */
    code span.ot { color: #007020; } /* Other */
    code span.pp { color: #bc7a00; } /* Preprocessor */
    code span.sc { color: #4070a0; } /* SpecialChar */
    code span.ss { color: #bb6688; } /* SpecialString */
    code span.st { color: #4070a0; } /* String */
    code span.va { color: #19177c; } /* Variable */
    code span.vs { color: #4070a0; } /* VerbatimString */
    code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
  </style>
  <link rel="stylesheet" href="../../reveal.js/dist/theme/black.css" id="theme">
</head>
<body>
  <div class="reveal">
    <div class="slides">

<section id="title-slide">
  <h1 class="title">Optimizing a CUDA application</h1>
  <p class="author">Rupert Nash, Kevin Stratford, Alan Gray</p>
</section>

<section>
<section id="introduction" class="title-slide slide level1">
<h1>Introduction</h1>

</section>
<section id="credits" class="slide level2">
<h2>Credits</h2>
<p>Exercise content created by EPCC, The University of Edinburgh.
Documentation and source code copyright The University of Edinburgh
2019. Lab style and template created by NVIDIA, see <a
href="https://nvidia.qwiklab.com/"
class="uri">https://nvidia.qwiklab.com/</a>.</p>
</section>
<section id="purpose" class="slide level2">
<h2>Purpose</h2>
<p>In this self-paced, hands-on lab, we will take an existing CUDA
application and go through several optimization steps, measuring the
performance benefits of each. We will see the importance of minimizing
data transfer, enabling coalesced memory access, and tuning the parallel
decomposition.</p>
</section>
<section id="getting-the-source" class="slide level2">
<h2>Getting the source</h2>
<p>As before the code can be cloned from GitHub:</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode sh"><code class="sourceCode bash"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="fu">git</span> clone https://github.com/EPCCed/APT-CUDA.git</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="bu">cd</span> APT-CUDA/exercises/cuda-reconstruct</span></code></pre></div>
<p>Or you can use your existing repo (update with
<code>git pull</code>)</p>
</section></section>
<section>
<section id="application" class="title-slide slide level1">
<h1>Application</h1>

</section>
<section id="introduction-1" class="slide level2">
<h2>Introduction</h2>
<p>This exercise involves performing a series of optimizations to an
existing CUDA application.</p>
</section>
<section class="slide level2">

<p>You start with an image that looks like this:</p>
<p><img data-src="images/input.jpg" style="width:50.0%" /></p>
</section>
<section class="slide level2">

<p>Which has been generated from the original:</p>
<p><img data-src="images/EDINB00034_2048x2048.jpg"
style="width:50.0%" /></p>
</section>
<section class="slide level2">

<p>First is an image of Edinburgh Castle, processed such that the edges
between light and dark areas replace the original picture.</p>
</section>
<section id="your-job-reconstruct-the-initial-image"
class="slide level2">
<h2>Your job: reconstruct the initial image</h2>
<p>This is an artificial thing to do, but it mimics many scientific
applications (e.g. that solve systems of PDEs) since the algorithm is
iterative, requiring many successive <em>stencil</em> operations.</p>
</section>
<section class="slide level2">

<p>Each pixel of the new image <span class="math inline">\(M\)</span> is
generated based on its neighboring pixel values and the original edge
data <span class="math inline">\(E\)</span> by repeatedly performing the
following update:</p>
<p><span class="math display">\[
M_{i,j} = \frac{M_{i-1,j} + M_{i+1,j} + M_{i,j-1} + M_{i,j+1} -
E_{i,j}}{4}
\]</span></p>
<p>The more iterations, the better the reconstruction (although for
simplicity we work in greyscale rather than colour).</p>
</section></section>
<section>
<section id="getting-started" class="title-slide slide level1">
<h1>Getting started</h1>

</section>
<section id="run-the-original" class="slide level2">
<h2>Run the original</h2>
<p>You are provided with a working but slow CUDA implementation of the
reconstruction algorithm.</p>
<p>First of all, let’s compile and run the code. The code is set up to
run the algorithm on both the GPU and the CPU. It compares the outputs
from the two runs to verify correctness, and then displays timings for
each run.</p>
</section>
<section id="build-with-make" class="slide level2">
<h2>Build with make</h2>
<p>Choose to work with either C or Fortran</p>
<p>C:</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode sh"><code class="sourceCode bash"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="bu">cd</span> src_c</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="ex">module</span> load nvidia/nvhpc/22.11</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="fu">make</span></span></code></pre></div>
<p>Fortran:</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode sh"><code class="sourceCode bash"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="bu">cd</span> src_fortran</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="ex">module</span> load nvidia/nvhpc/22.11</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="fu">make</span></span></code></pre></div>
</section>
<section id="run-on-the-batch-system" class="slide level2">
<h2>Run on the batch system</h2>
<p>To run, you need to know your budget code. To do this, you need to
know your budget code - you can check by logging into SAFEm navigating
to the relevant Cirrus login account and checking which budgets it can
access.</p>
<p>Submit the job with:</p>
<pre><code>sbatch --account &lt;YOUR BUDGET CODE&gt; submit.sh</code></pre>
<p>Query SLURM for your jobs:</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode sh"><code class="sourceCode bash"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="ex">squeue</span> <span class="at">-u</span> <span class="va">$USER</span></span></code></pre></div>
<p>During on campus tutorials we have reserved one node (4 GPUs) for the
use of the class. You can access this by editing the SLURM script or
adding extra options to the <code>sbatch</code> command:</p>
<pre><code>sbatch --account d171 --qos=reservation
--reservation=&lt;reservation ID&gt;</code></pre>
</section>
<section id="view-the-resulting-image" class="slide level2">
<h2>View the resulting image</h2>
<p>On Cirrus you can either:</p>
<ol type="1">
<li>View directly with <code>display</code> if you have set up X11
forwarding when you connected</li>
</ol>
<div class="sourceCode" id="cb7"><pre class="sourceCode sh"><code class="sourceCode bash"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="ex">module</span> load ImageMagick</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a><span class="ex">display</span> output.pgm</span></code></pre></div>
<ol start="2" type="1">
<li>Use <code>convert</code> to turn the output file into e.g. a jpg or
png file and then copy it to your client</li>
</ol>
<div class="sourceCode" id="cb8"><pre class="sourceCode sh"><code class="sourceCode bash"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="ex">module</span> load ImageMagick</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a><span class="ex">convert</span> output.pgm output.jpg</span></code></pre></div>
</section>
<section class="slide level2">

<p>Hopefully you can see that the picture is starting to become clearer.
As the algorithm is iterative, there is a loop in the main routine that
invokes the kernel <span class="math inline">\(N=100\)</span> times.</p>
<p>Increasing N will increase the quality of the reconstruction, but
please don’t do this during the lab!</p>
</section>
<section class="slide level2">

<p>If you were to run for 10 million iterations, the resulting image
would look like this:</p>
<p><img data-src="images/output10M.jpg" style="width:50.0%" /></p>
</section></section>
<section>
<section id="optimising" class="title-slide slide level1">
<h1>Optimising</h1>

</section>
<section id="important-note" class="slide level2">
<h2>Important note!</h2>
<p>Keep a note of the run times of your jobs and what you have changed
each time!</p>
</section>
<section id="section" class="slide level2">
<h2></h2>
<p>Now it’s time to optimise the code and improve on the GPU timing
printed when you ran the code, by editing the source code.</p>
<p>Note that a one pixel wide <em>halo</em> region of zeroes is added to
each edge of the various image-data arrays; this simplifies the
computation as it allows the edge pixels to be treated in the same
manner as other pixels.</p>
<p>Note: The edge array, which holds the original edge data, does not
have require a halo.</p>
</section>
<section id="profiling" class="slide level2">
<h2>Profiling</h2>
<p>You can (should?) profile your code as you optimise it. There are
basic submission scripts for both Nsight Systems
(<code>profile-nsys.sh</code>) and Compute
(<code>profile-ncomp.sh</code>) included. Once you have run these, you
can transfer the files to your workstation and analyse them with the
GUIs (available from NVIDIA
https://developer.nvidia.com/tools-overview).</p>
</section>
<section id="minimizing-data-transfer" class="slide level2">
<h2>Minimizing Data Transfer</h2>
<p>A challenge with GPUs and other accelerators is that transferring
data between host memory and device memory is often relatively slow. An
important optimization technique involves minimise the amount of data
that is transferred between host and device.</p>
</section>
<section class="slide level2">

<p>Notice that in the main loop in <code>reconstruct.cu</code> (C) or
<code>reconstruct.cuf</code> (Fortran), the data is copied from GPU
memory to host memory and then back to GPU memory at each iteration.
This is not in fact necessary; with the exception of the final iteration
when the data must be copied back to the host, it is going to be
processed on the GPU again in the next iteration. Therefore, we can
optimise manipulating the GPU memory directly without expensive
transfers.</p>
</section>
<section class="slide level2">

<p>We can simply copy the output array directly to the input array after
each iteration.</p>
<p>In order to do this you will need to:</p>
</section>
<section id="c" class="slide level2">
<h2>C</h2>
<ul>
<li><p>Remove the <code>cudaMemcpy</code> calls from inside the main
loop</p></li>
<li><p>Replace them with a <code>cudaMemcpy</code> call to copy,
directly on the device, from <code>d_output</code> to
<code>d_input</code></p></li>
<li><p>Add a new <code>cudaMemcpy</code> call after the end of the loop
(in between the two calls to <code>get_current_time()</code>) to copy
the final result back from the GPU to the <code>output</code> buffer in
host memory.</p></li>
</ul>
</section>
<section id="fortran" class="slide level2">
<h2>Fortran</h2>
<ul>
<li><p>Remove the assignments to <code>output</code> (from
<code>d_output</code>) and to <code>d_input</code> (from
<code>output</code>), inside the main loop</p></li>
<li><p>Replace them with an assignment directly from
<code>d_output</code> to <code>d_input</code></p></li>
<li><p>Add a new assignment after the end of the loop (in between the
two calls to <code>cpu_time()</code>) to copy the final result back from
the GPU to the <code>output</code> buffer in host memory</p></li>
</ul>
</section>
<section class="slide level2">

<p>Once you have made these changes, compile and run the code again as
above and take note of the time taken by the GPU version.</p>
<p>How does it compare to the previous timing?</p>
</section></section>
<section>
<section id="enabling-coalesced-memory-accesses"
class="title-slide slide level1">
<h1>Enabling Coalesced Memory Accesses</h1>

</section>
<section id="reminder" class="slide level2">
<h2>Reminder</h2>
<p>The GPU performs best when consecutive CUDA threads access
consecutive memory locations, allowing memory <em>coalescing</em>.</p>
</section>
<section id="c-1" class="slide level2">
<h2>C</h2>
<p>For the kernel in <code>reconstruct_kernels.cu</code>, it can be seen
that consecutive threads correspond to consecutive rows of the image,
but consecutive memory locations instead correspond to consecutive
columns. The threads are not reading from consecutive locations.</p>
</section>
<section id="fortran-1" class="slide level2">
<h2>Fortran</h2>
<p>For the kernel in <code>reconstruct_kernels.cuf</code>, it can be
seen that consecutive threads correspond to consecutive columns of the
image, but consecutive memory locations instead correspond to
consecutive rows. The threads are not reading from consecutive
locations.</p>
</section>
<section id="what-to-do" class="slide level2">
<h2>What to do</h2>
<ul>
<li><p>Update the kernel such that the role of the image row and column
is reversed, in relation to how pixels are assigned to CUDA
threads.</p></li>
<li><p>Since the image is perfectly square, you will not need to change
the way the kernel is launched.</p></li>
<li><p>How does the performance compare to the previous
version?</p></li>
</ul>
</section></section>
<section>
<section id="improving-occupancy" class="title-slide slide level1">
<h1>Improving Occupancy</h1>
<p>You should hopefully have seen a noticeable improvement in
performance as a result of the changes you made to reduce data transfers
between the host and the device and to enable coalescing. However, the
current solution is still sub-optimal as it will not create sufficient
threads to utilise all the SMs on the GPU - it has low
<em>occupancy</em>.</p>
</section>
<section class="slide level2">

<p>GPU codes typically run best when there are many threads running in
parallel, each doing a small part of the work. We can achieve this with
our image processing code by using a thread for each pixel of the image,
rather than for each row or column as before. CUDA supports 1-, 2- or
3-dimensional decompositions. A 2D decomposition maps most naturally
onto the pixels of an image.</p>
</section>
<section class="slide level2">

<ul>
<li><p>Update your both your kernel, and the code responsible for
specifying the decomposition such that that a 2D decomposition is over
both rows and columns.</p></li>
<li><p>The original code uses 256 threads per block in a 1D CUDA
decomposition. Replace this with 16 threads in each of the X and Y
directions of the 2D CUDA decomposition, to give a total of 256 threads
per block. Ensure that the number of blocks is specified appropriately
in each direction.</p></li>
<li><p>Ensure that you retain memory coalescing!</p></li>
</ul>
<p>Measure performance and compare to the previous versions.</p>
</section></section>
<section>
<section id="investigating-grid-and-block-sizes"
class="title-slide slide level1">
<h1>Investigating Grid and Block Sizes</h1>

</section>
<section class="slide level2">

<p>Once you have the 2D kernel working correctly, you can try altering
certain parameters and see what effect this has on its performance. In
particular, you can investigate the effects of different grid and block
sizes.</p>
<p>How does changing the grid and block sizes affect the total
runtime?</p>
</section></section>
    </div>
  </div>

  <script src="../../reveal.js/dist/reveal.js"></script>

  <!-- reveal.js plugins -->
  <script src="../../reveal.js/plugin/notes/notes.js"></script>
  <script src="../../reveal.js/plugin/search/search.js"></script>
  <script src="../../reveal.js/plugin/zoom/zoom.js"></script>
  <script src="../../reveal.js/plugin/math/math.js"></script>

  <script>

      // Full list of configuration options available at:
      // https://revealjs.com/config/
      Reveal.initialize({
        // Display controls in the bottom right corner
        controls: true,

        // Help the user learn the controls by providing hints, for example by
        // bouncing the down arrow when they first encounter a vertical slide
        controlsTutorial: true,

        // Determines where controls appear, "edges" or "bottom-right"
        controlsLayout: 'bottom-right',

        // Visibility rule for backwards navigation arrows; "faded", "hidden"
        // or "visible"
        controlsBackArrows: 'faded',

        // Display a presentation progress bar
        progress: true,

        // Display the page number of the current slide
        slideNumber: false,

        // 'all', 'print', or 'speaker'
        showSlideNumber: 'all',

        // Add the current slide number to the URL hash so that reloading the
        // page/copying the URL will return you to the same slide
        hash: true,

        // Start with 1 for the hash rather than 0
        hashOneBasedIndex: false,

        // Flags if we should monitor the hash and change slides accordingly
        respondToHashChanges: true,

        // Push each slide change to the browser history
        history: false,

        // Enable keyboard shortcuts for navigation
        keyboard: true,

        // Enable the slide overview mode
        overview: true,

        // Disables the default reveal.js slide layout (scaling and centering)
        // so that you can use custom CSS layout
        disableLayout: false,

        // Vertical centering of slides
        center: true,

        // Enables touch navigation on devices with touch input
        touch: true,

        // Loop the presentation
        loop: false,

        // Change the presentation direction to be RTL
        rtl: false,

        // see https://revealjs.com/vertical-slides/#navigation-mode
        navigationMode: 'default',

        // Randomizes the order of slides each time the presentation loads
        shuffle: false,

        // Turns fragments on and off globally
        fragments: true,

        // Flags whether to include the current fragment in the URL,
        // so that reloading brings you to the same fragment position
        fragmentInURL: true,

        // Flags if the presentation is running in an embedded mode,
        // i.e. contained within a limited portion of the screen
        embedded: false,

        // Flags if we should show a help overlay when the questionmark
        // key is pressed
        help: true,

        // Flags if it should be possible to pause the presentation (blackout)
        pause: true,

        // Flags if speaker notes should be visible to all viewers
        showNotes: false,

        // Global override for autoplaying embedded media (null/true/false)
        autoPlayMedia: null,

        // Global override for preloading lazy-loaded iframes (null/true/false)
        preloadIframes: null,

        // Number of milliseconds between automatically proceeding to the
        // next slide, disabled when set to 0, this value can be overwritten
        // by using a data-autoslide attribute on your slides
        autoSlide: 0,

        // Stop auto-sliding after user input
        autoSlideStoppable: true,

        // Use this method for navigation when auto-sliding
        autoSlideMethod: null,

        // Specify the average time in seconds that you think you will spend
        // presenting each slide. This is used to show a pacing timer in the
        // speaker view
        defaultTiming: null,

        // Enable slide navigation via mouse wheel
        mouseWheel: false,

        // The display mode that will be used to show slides
        display: 'block',

        // Hide cursor if inactive
        hideInactiveCursor: true,

        // Time before the cursor is hidden (in ms)
        hideCursorTime: 5000,

        // Opens links in an iframe preview overlay
        previewLinks: false,

        // Transition style (none/fade/slide/convex/concave/zoom)
        transition: 'slide',

        // Transition speed (default/fast/slow)
        transitionSpeed: 'default',

        // Transition style for full page slide backgrounds
        // (none/fade/slide/convex/concave/zoom)
        backgroundTransition: 'fade',

        // Number of slides away from the current that are visible
        viewDistance: 3,

        // Number of slides away from the current that are visible on mobile
        // devices. It is advisable to set this to a lower number than
        // viewDistance in order to save resources.
        mobileViewDistance: 2,

        math: {
          mathjax: 'https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js',
          config: 'TeX-AMS_HTML-full',
          tex2jax: {
            inlineMath: [['\\(','\\)']],
            displayMath: [['\\[','\\]']],
            balanceBraces: true,
            processEscapes: false,
            processRefs: true,
            processEnvironments: true,
            preview: 'TeX',
            skipTags: ['script','noscript','style','textarea','pre','code'],
            ignoreClass: 'tex2jax_ignore',
            processClass: 'tex2jax_process'
          },
        },

        // reveal.js plugins
        plugins: [
          RevealMath,
          RevealNotes,
          RevealSearch,
          RevealZoom
        ]
      });
    </script>
    </body>
</html>
